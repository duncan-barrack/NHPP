# Classification and clustering for samples of event time data using non-homogeneous Poisson process models
This repository contains Matab code accompaining the paper ... .

# Software and version numbers
The code has been tested on Matlab R2014a (see www.mathworks.com) but should work with subsequent versions. 

# Description of files
* `NHPP_train.m - Obtains NHPP rate function estimates for training set for classification tasks. It has the following dependencies:-
 * NHPP_of.m - objective function to be optmimised
 * NHPP_con.m - constraint for the optimisation.
* NHPP_test.m - Obtains posteriori probabilities ofr test set data  

```
{
  "data": "data_dir",
  "features": "features_dir",
  "trained_model": "trained_model_dir",
  "submission": "submissions_dir"
}
```
* `data_dir` - Directory containing the downloaded competition data, e.g. `/home/user/Kaggle/BCI/data/`
* `features_dir` - Directory containing the features extracted from the raw data
* `trained_model_dir` - Directory containing the trained model
* `submissions_dir` - Directory submissions are written to

# Generating features
There are five Matlab scripts (`generate_meta_features.m`, `get_sess5_retrial_features.m`, `get_ave_amplitude_features.m`, `get_template_features1.m` and `get_template_features2.m`) which will generate the features used in my model. It's important to run these scripts in order as latter scripts use data produced by the former scripts. First run  `generate_meta_features.m`, then `get_sess5_retrial_features.m`, next `get_ave_amplitude_features.m`, then `get_template_features1.m` and finally `get_template_features2.m`.

To run `generate_meta_features.m`, simply type in the following from within Matlab 

`generate_meta_features`

or from within a terminal

`matlab -r "generate_meta_features"`

You will see the following printed to screen

```
Obtaining meta features for training set .....
Training set subject 2, session 1
Training set subject 2, session 2
```

After this script is complete, run the other scripts.

On my machine `generate_meta_features.m` takes ~5 mins to run, `get_sess5_retrial_features.m` ~1 min,  `get_ave_amplitude_features.m` a few days, `get_template_features1.m` a few days and `get_template_features2.m` ~1 day. There is a lot of scope to parallelise the code in `get_ave_amplitude_features.m`, `get_template_features1.m` and `get_template_features2.m` to speed things up.

All feature files are saved in the `features_dir` directory. For convenience I've placed the feature files generated by these scripts in [Dropbox] (https://www.dropbox.com/s/59zklz1s64qwenz/submit.tar.gz?dl=0), so there is no need to generate them from scratch.

# Description of features
* `generate_meta_features.m` - Generates 'meta' features including trial time-stamp, session number etc
* `get_sess5_retrial_features.m` - Attempts to infer the class labels for session 5 based on the time between feedback events  
* `get_ave_amplitude_features.m` - Mean EEG values in windows of various lengths and lags after feedback
* `get_template_features1.m` - All ERP signals labelled as positive feedback in the training set are averaged to create a 'positive feedback template' and similarly to create a 'negative feedback template'. Correlation, covariance, and euclidean distance between the templates and test ERP signals as used as the basis for features. 
* `get_template_features2.m` - As above except the features are based on the cross-correlation and cross-covariance.

# Fitting the model
As my final model is a weighted average of two models which use different feature sets, my code generates two models. To fit the models run the following in a terminal

`python train_model.py`

The following will appear on the screen
```
Model 1 (SVM, linear kernel, meta and average amplitude features), loading data ....
Model 1, applying subset selection ....
Model 1, training ....
Model 1, saving files ....
Model 2 (SVM, linear kernel, meta and template features), loading data ....
Model 2, training ....
Model 2, saving files ....
```

This takes ~ 15 mins to run on my machine. The two models, together with an array containing index values (used for feature sub-set selection), will be saved to the `trained_model_dir` directory. For convenience, all these files can be found in [Dropbox] (https://www.dropbox.com/s/59zklz1s64qwenz/submit.tar.gz?dl=0).

# Using the model to predict
Run `predict.py` as follows

`python predict.py`

You will see the following
```
Model 1, loading data ....
Model 1, making predictions....
Model 2, loading data ....
Model 2, making predictions....
Averaging models....
```

The predictions will be saved to the `submissions_dir` directory. I've put a copy of the predictions file in [Dropbox] (https://www.dropbox.com/s/59zklz1s64qwenz/submit.tar.gz?dl=0). 
